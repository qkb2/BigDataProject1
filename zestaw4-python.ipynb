{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861b6d4a-0de6-42ba-97a5-beef1f82f292",
   "metadata": {},
   "source": [
    "# Projekt Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b301ae8-ceff-4dbf-8d04-75bb4eb52480",
   "metadata": {},
   "source": [
    "# Wprowadzenie\n",
    "\n",
    "Wykorzystując ten notatnik jako szablon zrealizuj projekt Apache Spark zgodnie z przydzielonym zestawem. \n",
    "\n",
    "Kilka uwag:\n",
    "\n",
    "* Nie modyfikuj ani nie usuwaj paragrafów *markdown* w tym notatniku, chyba że wynika to jednoznacznie z instrukcji. \n",
    "* Istniejące paragrafy zawierające *kod* uzupełnij w razie potrzeby zgodnie z instrukcjami\n",
    "    - nie usuwaj ich\n",
    "    - nie usuwaj zawartych w nich instrukcji oraz kodu\n",
    "    - nie modyfikuj ich, jeśli instrukcje jawnie tego nie nakazują\n",
    "* Możesz dodawać nowe paragrafy zarówno zawierające kod jak i komentarze dotyczące tego kodu (markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d12f1-1013-4c74-b6aa-686ccfcbdd5c",
   "metadata": {},
   "source": [
    "# Treść projektu\n",
    "\n",
    "## Zestaw 4 – imdb-persons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc4ff6-4d43-49ed-a0d1-8b6988eaec16",
   "metadata": {},
   "source": [
    "# Zestaw 0 – wzorzec\n",
    "\n",
    "**Uwaga**\n",
    "\n",
    "- W ramach wzorca nie są spełnione żadne reguły projektu. \n",
    "- Brak konsekwencji w wykorzystaniu właściwego API w ramach poszczególnych części\n",
    "- Zadanie *misji głównej* polega na zliczeniu słówek.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e128e43-6cce-4ffa-9609-9fae4b164ae9",
   "metadata": {},
   "source": [
    "# Działania wstępne \n",
    "\n",
    "Uruchom poniższy paragraf, aby utworzyć obiekty kontekstu Sparka. Jeśli jest taka potrzeba dostosuj te polecenia. Pamiętaj o potrzebnych bibliotekach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo /opt/conda/miniconda3/bin/mamba install delta-spark==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fb1050-386f-4398-ba5a-b45f5065d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# Spark session & context\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"zestaw4\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695a354-52bc-4bba-8222-7121bf07ae90",
   "metadata": {},
   "source": [
    "W poniższym paragrafie uzupełnij polecenia definiujące poszczególne zmienne. \n",
    "\n",
    "Pamiętaj abyś:\n",
    "\n",
    "* w późniejszym kodzie, dla wszystkich cześci projektu, korzystał z tych zdefiniowanych zmiennych. Wykorzystuj je analogicznie jak parametry\n",
    "* przed ostateczną rejestracją projektu usunął ich wartości, tak aby nie pozostawiać w notatniku niczego co mogłoby identyfikować Ciebie jako jego autora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e883af01-7117-4faa-a840-7ff807a195d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pełna ścieżka do katalogu w zasobniku zawierającego podkatalogi `datasource1` i `datasource4` \n",
    "# z danymi źródłowymi\n",
    "input_dir = \"/home/jovyan/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601cc7a-3ed5-47e2-994f-ebec642049b5",
   "metadata": {},
   "source": [
    "Nie modyfikuj poniższych paragrafów. Wykonaj je i używaj zdefniowanych poniżej zmiennych jak parametrów Twojego programu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6167e297-01ed-463e-bb81-9104d7cf7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "# ścieżki dla danych źródłowych \n",
    "datasource1_dir = input_dir + \"/datasource1\"\n",
    "datasource4_dir = input_dir + \"/datasource4\"\n",
    "\n",
    "# nazwy i ścieżki dla wyników dla misji głównej \n",
    "# część 1 (Spark Core - RDD) \n",
    "rdd_result_dir = \"/tmp/output1\"\n",
    "\n",
    "# część 2 (Spark SQL - DataFrame)\n",
    "df_result_table = \"output2\"\n",
    "\n",
    "# część 3 (Pandas API on Spark)\n",
    "ps_result_file = \"/tmp/output3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36e0314-a4ac-4096-9e4b-23fd4a73e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "import os\n",
    "def remove_file(file):\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "remove_file(\"metric_functions.py\")\n",
    "remove_file(\"tools_functions.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b8e00-10ae-47dc-b623-d1dacbe9c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "import requests\n",
    "r = requests.get(\"https://jankiewicz.pl/bigdata/metric_functions.py\", allow_redirects=True)\n",
    "open('metric_functions.py', 'wb').write(r.content)\n",
    "r = requests.get(\"https://jankiewicz.pl/bigdata/tools_functions.py\", allow_redirects=True)\n",
    "open('tools_functions.py', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a433894-dc97-46f2-be51-9f40fa36894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "%run metric_functions.py\n",
    "%run tools_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3a9dc-ac3b-4316-abb9-365caa1d7185",
   "metadata": {},
   "source": [
    "Poniższe paragrafy mają na celu usunąć ewentualne pozostałości poprzednich uruchomień tego lub innych notatników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08091c72-937f-41c2-9afe-d1505862bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "# usunięcie miejsca docelowego dla część 1 (Spark Core - RDD) \n",
    "delete_dir(spark, rdd_result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e863c0-c824-47bd-b53a-ce3b1fd6d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "# usunięcie miejsca docelowego dla część 2 (Spark SQL - DataFrame) \n",
    "drop_table(spark, df_result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72956a1a-da48-4d2b-a07a-e03d56431d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "# usunięcie miejsca docelowego dla część 3 (Pandas API on Spark) \n",
    "remove_file(ps_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e423d4-92b8-4161-98da-1a867f86d780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14faf05b-6c52-4b02-b2e5-2ddb3f38c704",
   "metadata": {},
   "source": [
    "***Uwaga!***\n",
    "\n",
    "Uruchom poniższy paragraf i sprawdź czy adres, pod którym dostępny *Apache Spark Application UI* jest poprawny wywołując następny testowy paragraf. \n",
    "\n",
    "W razie potrzeby określ samodzielnie poprawny adres, pod którym dostępny *Apache Spark Application UI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32acf3d2-ec4e-469d-bb0b-5f260c2c8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adres URL, pod którym dostępny Apache Spark Application UI (REST API)\n",
    "# \n",
    "spark_ui_address = extract_host_and_port(spark, \"http://localhost:4041\")\n",
    "spark_ui_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2329e-1d7a-465f-a23b-333f95bf7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testowy paragraf\n",
    "test_metrics = get_current_metrics(spark_ui_address)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccca69-c577-440c-aa5c-c9df3a54e127",
   "metadata": {},
   "source": [
    "# Część 1 - Spark Core (RDD)\n",
    "\n",
    "## Misje poboczne\n",
    "\n",
    "W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af3440-983a-4cac-a8e7-4908b010947c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc37879-e0fa-4c4a-bd0d-4c01c3ecf38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d303a72b-4083-470e-b25d-3224360ee94f",
   "metadata": {},
   "source": [
    "## Misja główna \n",
    "\n",
    "Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037689d7-f0ee-4165-bef0-83fa7f3e8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "before_rdd_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23971c0-cec7-4ea8-befb-7f063dce863c",
   "metadata": {},
   "source": [
    "W poniższych paragrafach wprowadź **rozwiązanie** *misji głównej* oparte na *RDD API*. \n",
    "\n",
    "Pamiętaj o wydajności Twojego przetwarzania, *RDD API* tego wymaga. \n",
    "\n",
    "Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af00c41-02a9-4a85-b3c6-bc41098edbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load by textFile\n",
    "datasource1 = sc.textFile(datasource1_dir).map(lambda x: x.split(\"\\t\"))\n",
    "datasource4 = sc.textFile(datasource4_dir).map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "# map and normalize to performer\n",
    "ds1_rdd = datasource1.map(\n",
    "    lambda x: (\n",
    "        x[0],\n",
    "        x[2],\n",
    "        x[3],\n",
    "        \"performer\" if x[3] in {\"actor\", \"actress\", \"self\"} else x[3],\n",
    "    )\n",
    ")  # (tconst, nconst, role, role_normalized)\n",
    "\n",
    "ds4_rdd = datasource4.map(\n",
    "    lambda x: (x[0], x[1], x[4])\n",
    ")  # (nconst, primaryName, primaryProfession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7955a5f-386d-47a5-9f6a-3d93a906c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map-reduce-by-key to create sets of staff per movie, apply filters\n",
    "grouped_roles = (\n",
    "    ds1_rdd.map(lambda x: (x[0], {x[3]}))  # Start with a set for roles\n",
    "    .reduceByKey(lambda roles1, roles2: roles1.union(roles2))  # Merge sets of roles per key\n",
    "    .filter(lambda x: {\"performer\", \"director\"}.issubset(x[1]) and len(x[1]) > 3)  # Apply filters\n",
    ")\n",
    "full_cast_movies = grouped_roles.map(lambda x: (x[0], \"\")) # for join\n",
    "\n",
    "# get only full-cast roles\n",
    "full_cast_roles = full_cast_movies.join(ds1_rdd.map(lambda x: (x[0], (x[1], x[2]))))\n",
    "full_cast_roles_count = full_cast_roles.\\\n",
    "    map(lambda x: ((x[1][1][0], x[1][1][1]), 1)).reduceByKey(lambda a,b: a+b) # ((nconst, profession), movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b019b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter actor profession data\n",
    "actor_data = ds4_rdd.flatMap(\n",
    "    lambda row: [\n",
    "        (profession, 1)\n",
    "        for profession in row[2].split(\",\")\n",
    "        if profession and profession != \"miscellaneous\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# calculate the top4 professions, filter for the best 4\n",
    "top_professions = actor_data.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], False)\n",
    "top4_professions = top_professions.zipWithIndex().filter(lambda x: x[1] < 4).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5380508",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_cast_roles_count_tx = full_cast_roles_count.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
    "\n",
    "# movies per person are joined by join and not by filtering full_cast_roles_count_tx with top_professions array to not go beyond the API\n",
    "movies_per_person = top4_professions.\\\n",
    "    join(full_cast_roles_count_tx).\\\n",
    "    map(lambda x: (x[0], x[1][1][0], x[1][1][1], x[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3287ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranking and sorting, get top3 actors per category \n",
    "ranked = movies_per_person.groupBy(lambda x: x[0]).\\\n",
    "    mapValues(lambda rows: sorted(rows, key=lambda x: x[2], reverse=True)[:3]).\\\n",
    "    flatMap(lambda x: x[1])\n",
    "    \n",
    "semi_final_result = ranked.map(lambda x: (x[1], (x[0], x[2], x[3]))).\\\n",
    "    join(ds4_rdd.map(lambda x: (x[0], x[1]))).map(lambda x: (x[1][0][0], x[1][1], x[1][0][1], x[1][0][2]))\n",
    "    \n",
    "final_result_sorted = semi_final_result.sortBy(lambda x: (-x[3], -x[2])).map(lambda x: (x[0], x[1], x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d77fd7-1f15-4365-ae80-c902aeb55ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to pickle\n",
    "final_result_sorted.saveAsPickleFile(rdd_result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8b5ec-b799-4177-8e4a-80a583d995e7",
   "metadata": {},
   "source": [
    "Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4325d378-b145-4e8f-8d37-80a072b506c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "after_rdd_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28137d3d-6f0d-443f-97b8-38104aaced6d",
   "metadata": {},
   "source": [
    "# Część 2 - Spark SQL (DataFrame)\n",
    "\n",
    "## Misje poboczne\n",
    "\n",
    "W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d045dae-5826-4015-8833-564d356db1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7738406-c426-4238-b0fb-983f4585bc5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7e569f-5f6b-4a98-b177-1b6fb0fc3333",
   "metadata": {},
   "source": [
    "## Misja główna \n",
    "\n",
    "Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6329c04b-3e50-41a8-93f1-333ac0ea64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "before_df_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cfb0d-51b6-45bb-b173-ab8ac630d4f3",
   "metadata": {},
   "source": [
    "W poniższych paragrafach wprowadź **rozwiązanie** *misji głównej* swojego projektu oparte o *DataFrame API*. \n",
    "\n",
    "Pamiętaj o wydajności Twojego przetwarzania, *DataFrame API* nie jest w stanie wszystkiego \"naprawić\". \n",
    "\n",
    "Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca6e627-0ce5-4c48-b441-3bcc14e32f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, count, desc\n",
    "# loading data\n",
    "datasource1 = spark.read.option(\"sep\", \"\\t\").csv(datasource1_dir, inferSchema=True)\n",
    "datasource4 = spark.read.option(\"sep\", \"\\t\").csv(datasource4_dir, header=True, inferSchema=True)\n",
    "datasource1 = datasource1.toDF(\"tconst\", \"ordering\", \"nconst\", \"role\", \"job\", \"characters\")\n",
    "datasource4 = datasource4.toDF(\"nconst\", \"primaryName\", \"birthYear\", \"deathYear\", \"primaryProfession\", \"knownForTitles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcc4aaa9-8dc2-4726-871e-5e2450ba3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, size, array_contains, row_number, when\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# normalize roles to performer\n",
    "normalized_roles = datasource1.withColumn(\n",
    "    \"normalized_role\",\n",
    "    when(col(\"role\").isin(\"actor\", \"actress\", \"self\"), \"performer\").otherwise(col(\"role\"))\n",
    ")\n",
    "\n",
    "# aggregate roles to find movies with at least 4 staff members, at least one performer and director\n",
    "full_cast = normalized_roles.groupBy(\"tconst\").agg(collect_set(\"normalized_role\").alias(\"roles\")).\\\n",
    "    filter(\n",
    "        array_contains(col(\"roles\"), \"performer\") & \n",
    "        array_contains(col(\"roles\"), \"director\") & \n",
    "        (size(col(\"roles\")) > 3)\n",
    "    ).select(\"tconst\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only roles from full-cast movies\n",
    "full_cast_roles = full_cast.join(datasource1, \"tconst\").select(\"tconst\", \"nconst\", \"role\")\n",
    "full_cast_roles_count = full_cast_roles.groupBy(\"nconst\", \"role\").agg(count(\"tconst\").alias(\"movies\")).\\\n",
    "    select(\"nconst\", col(\"role\").alias(\"profession\"), \"movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter professions\n",
    "actor_data = datasource4.withColumn(\"profession\", explode(split(col(\"primaryProfession\"), \",\"))).\\\n",
    "    filter(col(\"profession\") != \"miscellaneous\")\n",
    "# best professions\n",
    "top_professions = actor_data.groupBy(\"profession\").agg(count(\"nconst\").alias(\"count\")).orderBy(desc(\"count\")).limit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7243ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movies per person are joined by join and not by filtering full_cast_roles_count with top_professions array to not go beyond the API\n",
    "movies_per_person = full_cast_roles_count.join(datasource4.select(\"nconst\", \"primaryName\"), \"nconst\").join(top_professions, \"profession\")\n",
    "# partition by profession\n",
    "window_spec = Window.partitionBy(\"profession\").orderBy(desc(\"movies\"))\n",
    "ranked_movies_per_person2 = movies_per_person.withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ")\n",
    "final_result = ranked_movies_per_person2.filter(col(\"rank\") <= 3).\\\n",
    "    select(\"profession\", \"primaryName\", \"movies\").orderBy(\"profession\", desc(\"movies\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45165cca-5197-4590-ba69-7541085147f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to delta table\n",
    "final_result.write.mode(\"overwrite\").format(\"delta\").saveAsTable(df_result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0797752-450e-4f8f-a1d4-93a890a62c3d",
   "metadata": {},
   "source": [
    "Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3647eae-2801-46ac-b43d-74e5bbfcab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "after_df_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed01aa-cc23-427e-84c8-e5b76b9323bb",
   "metadata": {},
   "source": [
    "# Część 3 - Pandas API on Spark\n",
    "\n",
    "Ta część to wyzwanie. W szczególności dla osób, które nie programują na co dzień w Pythonie, lub które nie nie korzystały do tej pory z Pandas API.  \n",
    "\n",
    "Powodzenia!\n",
    "\n",
    "## Misje poboczne\n",
    "\n",
    "W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140b666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0c78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a5184ce-cf42-4342-aeec-b56c30b66bbd",
   "metadata": {},
   "source": [
    "## Misja główna \n",
    "\n",
    "Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63fd8306-87e9-46f2-b622-d60693e3ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIE ZMIENIAĆ\n",
    "before_ps_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967f079-7106-4bd7-9d26-98ced2aeb43b",
   "metadata": {},
   "source": [
    "W poniższych paragrafach wprowadź **rozwiązanie** swojego projektu oparte o *Pandas API on Spark*. \n",
    "\n",
    "Pamiętaj o wydajności Twojego przetwarzania, *Pandas API on Spark* nie jest w stanie wszystkiego \"naprawić\". \n",
    "\n",
    "Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36abbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "# Loading data\n",
    "datasource1 = ps.read_csv(datasource1_dir, sep=\"\\t\", header=None)\n",
    "datasource1.columns = [\"tconst\", \"ordering\", \"nconst\", \"role\", \"job\", \"characters\"]\n",
    "\n",
    "datasource4 = ps.read_csv(datasource4_dir, sep=\"\\t\", header=0)\n",
    "datasource4.columns = [\"nconst\", \"primaryName\", \"birthYear\", \"deathYear\", \"primaryProfession\", \"knownForTitles\"]\n",
    "\n",
    "# Normalize roles to \"performer\"\n",
    "normalized_roles = datasource1.assign(\n",
    "    normalized_role=datasource1[\"role\"].apply(\n",
    "        lambda x: \"performer\" if x in [\"actor\", \"actress\", \"self\"] else x\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter for full-cast movies\n",
    "def filter_roles(roles):\n",
    "    return \"performer\" in roles and \"director\" in roles and len(roles) > 3\n",
    "\n",
    "roles_grouped = normalized_roles.groupby(\"tconst\").agg({\"normalized_role\": \"collect_set\"}).reset_index()\n",
    "roles_grouped.columns = [\"tconst\", \"roles\"]\n",
    "full_cast_movies = roles_grouped[roles_grouped[\"roles\"].apply(filter_roles)][[\"tconst\"]]\n",
    "\n",
    "# Select roles from full-cast movies\n",
    "full_cast_roles = full_cast_movies.merge(datasource1, on=\"tconst\")[[\"tconst\", \"nconst\", \"role\"]]\n",
    "full_cast_roles_count = full_cast_roles.groupby([\"nconst\", \"role\"]).agg({\"tconst\": \"count\"}).reset_index()\n",
    "full_cast_roles_count.columns = [\"nconst\", \"profession\", \"movies\"]\n",
    "\n",
    "# Filter professions\n",
    "actor_data = datasource4.assign(\n",
    "    profession=datasource4[\"primaryProfession\"].apply(lambda x: x.split(\",\") if isinstance(x, str) else [])\n",
    ").explode(\"profession\")\n",
    "actor_data = actor_data[actor_data[\"profession\"] != \"miscellaneous\"]\n",
    "\n",
    "# Determine top professions\n",
    "top_professions = actor_data.groupby(\"profession\").agg({\"nconst\": \"count\"}).reset_index()\n",
    "top_professions.columns = [\"profession\", \"count\"]\n",
    "top_professions = top_professions.sort_values(\"count\", ascending=False).head(4)\n",
    "\n",
    "# Combine roles with top professions\n",
    "movies_per_person = (\n",
    "    full_cast_roles_count\n",
    "    .merge(datasource4[[\"nconst\", \"primaryName\"]], on=\"nconst\", how=\"inner\")\n",
    "    .merge(top_professions[[\"profession\"]], on=\"profession\", how=\"inner\")\n",
    ")\n",
    "\n",
    "# get top3s\n",
    "movies_per_person_top3 = movies_per_person.groupby(\"profession\").apply(\n",
    "    lambda group: group.sort_values(by=[\"movies\"], ascending=[False]).head(3)\n",
    ")\n",
    "\n",
    "persons_dropped = movies_per_person_top3.reset_index(drop=True)\n",
    "persons_to_json = persons_dropped[[\"profession\", \"primaryName\", \"movies\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02917f4-e1f2-4fb4-8b53-8829fb3f0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = persons_to_json.head(50).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76e0d7f7-82f3-41d4-8267-cf288f2f6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(ps_result_file, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a0ec5-ab13-4e39-a572-e7adf8b8556a",
   "metadata": {},
   "source": [
    "Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "108bee2a-a847-4625-8e4a-939951ac9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIE ZMIENIAĆ\n",
    "after_ps_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e266b-b5cd-41d0-aeab-c1edc365910d",
   "metadata": {},
   "source": [
    "# Analiza wyników i wydajności *misji głównych*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b67111-62d0-4657-b158-1ed37db9ed96",
   "metadata": {},
   "source": [
    "## Część 1 - Spark Core (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc9900-7e0c-49ff-adba-e339f83ffe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie wyników z pliku pickle\n",
    "word_counts = sc.pickleFile(rdd_result_dir)\n",
    "\n",
    "# Wyświetlenie 50 pierwszych elementów\n",
    "result_sample = word_counts.take(50)\n",
    "for item in result_sample:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edae69-8062-4422-842f-d50bca0af9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract_metrics(after_rdd_metrics, before_rdd_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc730f1-4b5e-4a68-8a86-11768918fcf4",
   "metadata": {},
   "source": [
    "## Część 2 - Spark SQL (DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950a09d-045e-4143-a3cf-8ecc7c73ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(df_result_table)\n",
    "\n",
    "# Wyświetlenie 50 pierwszych rekordów\n",
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f344ed9-94c1-4d79-b839-1839548d8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract_metrics(after_df_metrics, before_df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063b46c-579d-4775-ba3f-837708279ea2",
   "metadata": {},
   "source": [
    "## Część 3 - Pandas API on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e31a2-fd31-40ca-be7b-b20b13dc38a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Odczytaj zawartość pliku JSON\n",
    "with open(ps_result_file, 'r') as file:\n",
    "    json_content = json.load(file)\n",
    "\n",
    "# Wyświetl zawartość\n",
    "print(json.dumps(json_content, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32788c91-3f8e-4fb1-8afc-5eb00938e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract_metrics(after_ps_metrics, before_ps_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
